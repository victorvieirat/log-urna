{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "\n",
    "sc = SparkSession.builder.appName(\"DataFrame\").config(\"spark.driver.memory\", \"8g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_interesse = [\n",
    "    \"Eleitor foi habilitado\",\n",
    "    \"Voto confirmado para [Deputado Federal]\",\n",
    "    \"Voto confirmado para [Deputado Estadual]\",\n",
    "    \"Voto confirmado para [Senador]\",\n",
    "    \"Voto confirmado para [Governador]\",\n",
    "    \"Voto confirmado para [Presidente]\",\n",
    "    \"Tecla indevida pressionada\",\n",
    "]\n",
    "regex_interesse = [\n",
    "    \"Município:\",\n",
    "    \"Zona Eleitoral:\",\n",
    "    \"Local de Votação:\",\n",
    "    \"Seção Eleitoral:\",\n",
    "]\n",
    "regex_interesse = \"|\".join(regex_interesse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "DATA_PATH = \"/app/data/\"\n",
    "input_file = DATA_PATH+\"decodificado/*\" \n",
    "\n",
    "source = sc.read.csv(input_file, sep=\"\\t\", header=False)\n",
    "source = source.withColumn(\"arquivo\", F.input_file_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"distinto = source.select('arquivo').distinct()\n",
    "amostra = distinto.sample(False, (100/distinto.count()),seed=1100)\n",
    "amostra = amostra.toPandas()['arquivo'].values\n",
    "source =  source.filter(F.col(\"arquivo\").isin(*amostra))\n",
    "print(f'Número de urnas selecionadas {len(amostra)}')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = source.withColumnRenamed(\"_c0\", \"data\").withColumnRenamed(\"_c4\", \"descricao\")\n",
    "df = df.select(\"data\", \"descricao\")\n",
    "df = df.withColumn(\"arquivo\", F.input_file_name())\n",
    "df = df.filter(\n",
    "    F.col(\"descricao\").isin(logs_interesse) | F.col(\"descricao\").rlike(regex_interesse)\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"data\", F.to_timestamp(F.col(\"data\"), \"dd/MM/yyyy HH:mm:ss\"))\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from unidecode import unidecode\n",
    "\n",
    "window = (\n",
    "    Window.orderBy(\"data\")\n",
    "    .partitionBy(\"arquivo\") #Paraleliza pelo arquivo\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    ")\n",
    "\n",
    "df_window = df\n",
    "for nome_extracao in [\"Município\", \"Zona Eleitoral\", \"Seção Eleitoral\"]:\n",
    "    nome_col = unidecode(nome_extracao).replace(\" \", \"_\")\n",
    "    df_window = df_window.withColumn(\n",
    "        nome_col,\n",
    "        F.when(\n",
    "            F.col(\"descricao\").contains(nome_extracao),\n",
    "            F.regexp_extract(F.col(\"descricao\"), rf\"{nome_extracao}: (\\d+)\", 1).cast(\n",
    "                \"integer\"\n",
    "            ),\n",
    "        ).otherwise(None),\n",
    "    )\n",
    "\n",
    "    df_window = df_window.withColumn(\n",
    "        nome_col, F.last(nome_col, ignorenulls=True).over(window)\n",
    "    )\n",
    "df_window.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teclas = df_window.filter(~F.col(\"descricao\").rlike(regex_interesse))\n",
    "df_teclas = df_teclas.withColumn(\n",
    "    \"erros_tecla\",\n",
    "    F.when(F.col(\"descricao\") == \"Tecla indevida pressionada\", 1).otherwise(0),\n",
    ")\n",
    "\n",
    "df_teclas = df_teclas.withColumn(\n",
    "    \"id_eleitor\", F.when(F.col(\"descricao\") == \"Eleitor foi habilitado\", 1).otherwise(0)\n",
    ")\n",
    "my_window = (\n",
    "    Window.orderBy(\"data\")\n",
    "    .partitionBy(\"arquivo\")\n",
    "    .rowsBetween(Window.unboundedPreceding, 0)\n",
    ")\n",
    "df_teclas = df_teclas.withColumn(\"id_eleitor\", F.sum(\"id_eleitor\").over(my_window))\n",
    "\n",
    "\n",
    "df_teclas.orderBy([\"arquivo\",\"data\"], ascending=[1,1]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_window = (\n",
    "    Window.orderBy(\"data\")\n",
    "    .partitionBy(\"arquivo\",\"id_eleitor\")\n",
    "    .rowsBetween(Window.unboundedPreceding, 0)\n",
    ")\n",
    "df_soma_ate_momento = df_teclas.withColumn(\"erros_ate_agora\", F.sum(\"erros_tecla\").over(my_window))\n",
    "df_soma_ate_momento.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agrupado_eleitor = df_soma_ate_momento.groupBy(\"arquivo\", \"id_eleitor\").agg({\"erros_tecla\": \"sum\"}).orderBy([\"arquivo\",\"id_eleitor\"], ascending=[1,1])\n",
    "agrupado_eleitor.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agrupado_eleitor.orderBy([\"sum(erros_tecla)\"], ascending=[0]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_soma_tecla = df_soma_ate_momento.join(agrupado_eleitor, on=['arquivo', 'id_eleitor'], how='left')\n",
    "df_soma_tecla = df_soma_tecla.drop(\"erros_tecla\")\n",
    "df_soma_tecla.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sem_tecla = df_soma_tecla.filter(\n",
    "    ~F.col(\"descricao\").isin([\"Tecla indevida pressionada\"])\n",
    ")\n",
    "df_sem_tecla.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy(\"arquivo\", \"id_eleitor\").orderBy(\"data\")\n",
    "df_tempo_segundos = (\n",
    "    df_sem_tecla.withColumn(\"lag_tempo\", F.lag(df_teclas[\"data\"], 1).over(w))\n",
    "    .withColumn(\n",
    "        \"tempo(segundos)\",\n",
    "        (\n",
    "            F.unix_timestamp(\n",
    "                df_teclas[\"data\"],\n",
    "            )\n",
    "            - F.unix_timestamp(F.col(\"lag_tempo\"))\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "df_tempo_segundos.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"arquivo\", \"id_eleitor\").orderBy(\"data\")\n",
    "\n",
    "\n",
    "df_with_diff = df_tempo_segundos.withColumn(\"erro_nesse_cargo\", F.col(\"erros_ate_agora\") - F.lag(F.col(\"erros_ate_agora\")).over(window_spec))\n",
    "df_with_diff.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_with_diff.drop(\"lag_tempo\")\n",
    "df_final = df_final.filter(\n",
    "    ~F.col(\"descricao\").isin([\"Eleitor foi habilitado\"])\n",
    ")\n",
    "df_final = df_final.withColumn(\"descricao\", F.regexp_replace(\"descricao\", r\"(\\[|\\]|\\bVoto confirmado para\\b)\", \"\"))\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.write.mode('overwrite').parquet(DATA_PATH+\"dados_log_urna\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
